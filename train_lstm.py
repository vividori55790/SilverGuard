# SilverGuard/main.py
import cv2
import torch
import joblib
import numpy as np
import time
import os
import datetime
import math
from collections import deque
from ultralytics import YOLO
import utils
from models import FallLSTM  # models.pyì—ì„œ ë¶ˆëŸ¬ì˜´

# ==========================================
# [ì„¤ì •]
# ==========================================
SEQUENCE_LENGTH = 30  # í•™ìŠµ ë•Œ ì„¤ì •í•œ ìœˆë„ìš° í¬ê¸° (30í”„ë ˆìž„)
INPUT_SIZE = 54       # (17ê°œ í‚¤í¬ì¸íŠ¸ * 3) + (ì†ë„ ë“± ì¶”ê°€ í”¼ì²˜ 3ê°œ)
HIDDEN_SIZE = 64
NUM_LAYERS = 2

def calculate_angle(p1, p2):
    dx = p2[0] - p1[0]
    dy = p2[1] - p1[1]
    return abs(math.degrees(math.atan2(dx, dy)))

def main():
    print("ðŸš€ SilverGuard: LSTM ê¸°ë°˜ ì‹¤ì‹œê°„ ë‚™ìƒ ê°ì§€ ì‹œìž‘")
    utils.ensure_dirs()

    # 1. ìž¥ì¹˜ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"   - ê°€ì† ìž¥ì¹˜: {device}")

    # 2. ëª¨ë¸ ë¡œë“œ
    print("   - ëª¨ë¸ ë¡œë”© ì¤‘...")
    yolo_model = YOLO(utils.YOLO_MODEL_PATH)
    
    # LSTM ëª¨ë¸ êµ¬ì¡° ì´ˆê¸°í™” ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
    lstm_model = FallLSTM(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(device)
    lstm_path = os.path.join(utils.MODEL_DIR, 'fall_lstm.pth')
    
    if not os.path.exists(lstm_path):
        print(f"âŒ ì˜¤ë¥˜: í•™ìŠµëœ LSTM ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤ ({lstm_path}). train_lstm.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return
        
    lstm_model.load_state_dict(torch.load(lstm_path, map_location=device))
    lstm_model.eval() # í‰ê°€ ëª¨ë“œ ì „í™˜

    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ (í•™ìŠµ ë°ì´í„°ì™€ ë˜‘ê°™ì€ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”í•´ì•¼ í•¨)
    scaler_path = os.path.join(utils.MODEL_DIR, 'scaler.pkl')
    if not os.path.exists(scaler_path):
        print("âŒ ì˜¤ë¥˜: ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. train_lstm.py ì‹¤í–‰ ì‹œ ìƒì„±ë©ë‹ˆë‹¤.")
        return
    scaler = joblib.load(scaler_path)

    # 3. ì˜ìƒ ì†ŒìŠ¤ ì„¤ì •
    test_video_path = os.path.join(utils.VIDEO_DIR, utils.TEST_VIDEO_NAME)
    # íŒŒì¼ì´ ìžˆìœ¼ë©´ íŒŒì¼ ì‚¬ìš©, ì—†ìœ¼ë©´ 0ë²ˆ ì¹´ë©”ë¼(ì›¹ìº )
    video_source = test_video_path if os.path.exists(test_video_path) else 0
    cap = cv2.VideoCapture(video_source)
    
    if not cap.isOpened():
        print("âŒ ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 4. ì‹¤ì‹œê°„ ë°ì´í„° ë²„í¼ (ìµœê·¼ 30í”„ë ˆìž„ ì €ìž¥ìš©)
    # ì‚¬ëžŒ IDë³„ë¡œ ë²„í¼ë¥¼ ê´€ë¦¬í•´ì•¼ ì—¬ëŸ¬ ëª…ì¼ ë•Œ ì•ˆ ì„žì´ì§€ë§Œ, 
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨ížˆ 'í™”ë©´ ë‚´ ê°€ìž¥ í¬ê²Œ ìž¡ížŒ 1ëª…'ë§Œ ì¶”ì í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
    frame_buffer = deque(maxlen=SEQUENCE_LENGTH)
    
    # ì´ì „ í”„ë ˆìž„ ì •ë³´ (ì†ë„ ê³„ì‚°ìš©)
    prev_head_y = None
    prev_angle = None
    
    # ë‚™ìƒ ìƒíƒœ ê´€ë¦¬
    is_fall_state = False
    fall_start_time = None
    
    print("âœ… ê°ì‹œ ì‹œìž‘ (Ctrl+Cë¡œ ì¢…ë£Œ)")

    while True:
        ret, frame = cap.read()
        if not ret:
            print("ì˜ìƒ ì¢…ë£Œ/ë£¨í”„")
            # ë¬´í•œ ë£¨í”„ ì›í•˜ë©´ ì•„ëž˜ ì£¼ì„ í•´ì œ
            # cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
            # frame_buffer.clear()
            # prev_head_y = None
            # continue
            break

        # (ì„ íƒ) í…ŒìŠ¤íŠ¸ ì˜ìƒì¼ ê²½ìš° ì˜¤ë¥¸ìª½ ì ˆë°˜ë§Œ ìžë¥´ê¸° (UR Fall ë°ì´í„°ì…‹ íŠ¹ì„±)
        # ì›¹ìº  ì‚¬ìš© ì‹œì—ëŠ” ì´ ë¶€ë¶„ì„ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”.
        if utils.CROP_RIGHT_HALF:
             h, w, _ = frame.shape
             frame = frame[:, w//2:]

        # YOLO ì¶”ë¡ 
        results = yolo_model(frame, verbose=False)
        
        # ì‚¬ëžŒì´ ê°ì§€ë˜ì—ˆëŠ”ì§€ í™•ì¸
        detected = False
        
        for r in results:
            if r.keypoints is None or len(r.keypoints) == 0: continue

            # ê°€ìž¥ ì‹ ë¢°ë„ê°€ ë†’ê±°ë‚˜ í¬ê²Œ ìž¡ížŒ ì‚¬ëžŒ 1ëª…ë§Œ ì„ íƒ (ë‹¨ìˆœí™”)
            # ì‹¤ì œ ë°°í¬íŒì—ì„œëŠ” ID Tracking(ByteTrack ë“±)ì´ í•„ìš”í•  ìˆ˜ ìžˆìŒ
            kpts = r.keypoints.xyn[0].cpu().numpy() # (17, 2)
            confs = r.keypoints.conf[0].cpu().numpy() # (17,)
            bbox = r.boxes.xyxy[0].cpu().numpy() # [x1, y1, x2, y2]
            
            if len(kpts) == 17:
                detected = True
                
                # --- Feature Engineering (preprocess_urfall_velocity.pyì™€ ë™ì¼ ë¡œì§) ---
                head_y = kpts[0][1]
                shoulder_mid = (kpts[5] + kpts[6]) / 2
                hip_mid = (kpts[11] + kpts[12]) / 2
                current_angle = calculate_angle(shoulder_mid, hip_mid)
                
                if prev_head_y is not None:
                    head_velocity = (head_y - prev_head_y) * 30
                    angle_velocity = (current_angle - prev_angle) * 30
                else:
                    head_velocity = 0
                    angle_velocity = 0
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸
                prev_head_y = head_y
                prev_angle = current_angle
                
                # ìž…ë ¥ ë°ì´í„° ë²¡í„° ìƒì„± (54ì°¨ì›)
                row = []
                # 1. Keypoints (x, y, conf) - 51ê°œ
                for i in range(17):
                    row.extend([kpts[i][0], kpts[i][1], confs[i]])
                # 2. Derived Features - 3ê°œ
                row.extend([head_velocity, angle_velocity, current_angle])
                
                # ë²„í¼ì— ì¶”ê°€
                frame_buffer.append(row)
                
                # --- LSTM ì¶”ë¡  (ë°ì´í„°ê°€ 30í”„ë ˆìž„ ì°¼ì„ ë•Œë§Œ ìˆ˜í–‰) ---
                status_text = "Analyzing..."
                color = (0, 255, 0) # Green
                
                if len(frame_buffer) == SEQUENCE_LENGTH:
                    # (1, 30, 54) í˜•íƒœë¡œ ë³€í™˜
                    input_seq = np.array(frame_buffer) # (30, 54)
                    
                    # ìŠ¤ì¼€ì¼ë§ ì ìš© (í•™ìŠµ ë•Œ 2ì°¨ì›ìœ¼ë¡œ íŽ´ì„œ í–ˆìœ¼ë¯€ë¡œ ë˜‘ê°™ì´)
                    input_seq_2d = input_seq.reshape(-1, INPUT_SIZE)
                    input_seq_scaled = scaler.transform(input_seq_2d)
                    input_tensor = torch.tensor(input_seq_scaled, dtype=torch.float32).unsqueeze(0).to(device)
                    
                    # ì˜ˆì¸¡
                    with torch.no_grad():
                        output = lstm_model(input_tensor)
                        prob = torch.softmax(output, dim=1)
                        pred_cls = torch.argmax(prob, dim=1).item()
                        confidence = prob[0][pred_cls].item()
                    
                    # ê²°ê³¼ ì²˜ë¦¬
                    if pred_cls == 1 and confidence > 0.7: # ë‚™ìƒ(1)
                        status_text = f"FALL DETECTED ({confidence*100:.1f}%)"
                        color = (0, 0, 255) # Red
                        
                        if not is_fall_state:
                            is_fall_state = True
                            fall_start_time = time.time()
                            print(f"âš ï¸ ë‚™ìƒ ê°ì§€! - {status_text}")
                            
                            # ì¦‰ì‹œ ìº¡ì²˜ ì €ìž¥
                            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                            save_path = os.path.join(utils.ALERT_DIR, f"FALL_LSTM_{timestamp}.jpg")
                            cv2.imwrite(save_path, frame)
                            
                    else:
                        status_text = "Normal"
                        is_fall_state = False
                
                # ì‹œê°í™”
                cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2)
                cv2.putText(frame, status_text, (int(bbox[0]), int(bbox[1]-10)), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
        
        if not detected:
            # ì‚¬ëžŒì´ ì—†ìœ¼ë©´ ë²„í¼ ì´ˆê¸°í™” (ë‹¤ë¥¸ ì‚¬ëžŒì´ ë“¤ì–´ì˜¤ë©´ ì„žì´ë¯€ë¡œ)
            if len(frame_buffer) > 0: frame_buffer.clear()
            prev_head_y = None
            
    cap.release()
    print("ì‹œìŠ¤í…œ ì¢…ë£Œ.")

if __name__ == '__main__':
    main()